---
Title: Day 14 of 30 Day Of ML Code Event
Author: Rajendrasinh Parmar
Date: August 15, 2021
---

# Day 14 of 30 Days Of ML Code

## Intermediate Machine Learning Lesson 6

### XGBoost(Extreme Gradient Boosting)

In this tutorial, you will learn how to build and optimize models with gradient boosting. This method dominates many Kaggle competitions and achieves state-of-the-art results on a variety of datasets.

We will use xgboost library for the purpose of the lesson. scikit-learn also has a version of gradient boosting. However, xgboost has some technical advantages so we will use that.

Follow the notebook [XGBoost](./xgboost.ipynb) for lesson details.

#### Exercise

As a part of the exercise for the sixth lesson we have predicted the values of house prices using the xgboost regressor (gradient boosting).

The associated exercise with the sixth lesson of the course is provided in [Exercise: XGBoost](./exercise-xgboost.ipynb)

## Intermediate Machine Learning Lesson 7

### Data Leakage

In this tutorial, you will learn what data leakage is and how to prevent it. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in subtle and dangerous ways. So, this is one of the most important concepts for practicing data scientists.

There are mainly 2 kinds of data leakage.
1. target leakage
2. train-test contamination

Follow the notebook [Data Leakage](./data-leakage.ipynb) for lesson details.

#### Exercise

As a part of the exercise for the seventh lesson we have learned about data leakage and how to identify the leakage using different examples.

The associated exercise with the seventh lesson of the course is provided in [Exercise: Data Leakage](./exercise-data-leakage.ipynb)